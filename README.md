# EXP 5: COMPARATIVE ANALYSIS OF DIFFERENT TYPES OF PROMPTING PATTERNS

## Aim
To test and compare how different pattern models respond to **naïve prompts** (broad/unstructured) versus **basic prompts** (clear, detailed, and structured) across multiple scenarios. The analysis will evaluate **quality, accuracy, and depth** of generated responses.

---

## AI Tools Required
- ChatGPT (or any LLM-based AI tool)

---

## Definition of Prompt Types
1. **Naïve Prompt**
   - Broad, vague, and unstructured.  
   - Provides minimal context, leaving interpretation open.  
   - Example: *“Tell me about AI.”*

2. **Basic Prompt**
   - Clear, detailed, and structured.  
   - Provides context, constraints, and specific requirements.  
   - Example: *“Explain Artificial Intelligence in simple terms, covering its definition, real-life applications, and benefits in education.”*

---

## Test Scenarios

### Scenario 1: Creative Story
- **Naïve Prompt:** *“Write me a story.”*  
- **Basic Prompt:** *“Write a 300-word short story about a robot who learns human emotions, with a moral lesson at the end.”*

---

### Scenario 2: Factual Question
- **Naïve Prompt:** *“What is photosynthesis?”*  
- **Basic Prompt:** *“Explain photosynthesis in detail, including the process, chemical equation, and its importance for plants and humans.”*

---

### Scenario 3: Summarization
- **Naïve Prompt:** *“Summarize World War II.”*  
- **Basic Prompt:** *“Summarize World War II in 200 words, highlighting the causes, major events, and outcomes.”*

---

### Scenario 4: Advice/Recommendation
- **Naïve Prompt:** *“How to study better?”*  
- **Basic Prompt:** *“Give me 5 practical and science-backed study tips for college students preparing for competitive exams.”*

---

## Experimental Results

| **Scenario**            | **Naïve Prompt (Output)**                                  | **Basic Prompt (Output)**                                                                     | **Comparison** |
|--------------------------|------------------------------------------------------------|-----------------------------------------------------------------------------------------------|----------------|
| **Creative Story**       | Generic story, often very short and unfocused.             | A structured story with a robot, emotions, clear moral lesson.                                | Basic prompt gave better quality, depth, and direction. |
| **Factual Question**     | Short, textbook-like definition of photosynthesis.         | Detailed explanation with process, chemical equation, and importance.                         | Basic prompt improved accuracy and depth. |
| **Summarization**        | Overly long/short, missing structure.                      | Concise, organized, and within word limit (causes, events, outcomes).                         | Basic prompt ensured clarity and relevance. |
| **Advice/Recommendation**| General advice like “study more, avoid distractions.”      | Practical, actionable, and science-backed strategies (Pomodoro, spaced repetition, etc.).      | Basic prompt improved usefulness and depth. |

---

## Analysis of Results
- **Naïve prompts** gave generic or incomplete responses.  
- **Basic prompts** consistently provided **higher quality, accuracy, and depth**.  
- In creative tasks, naïve prompts sometimes worked but lacked direction.  
- In factual, summarization, and advice-based tasks, **clear and structured prompts led to significantly better outputs**.  

---

## Summary of Findings
- Prompt clarity **directly impacts output quality**.  
- Structured prompts reduce ambiguity and guide the AI toward **relevant, precise, and richer responses**.  
- For **optimal results**, always:  
  1. Provide context.  
  2. Specify format/constraints (word count, style, steps, etc.).  
  3. Clearly state the desired focus or outcome.  

---

## Result
The experiment on comparative analysis of prompting patterns was **executed successfully**.
